{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import argparse\n",
    "import time\n",
    "import datetime\n",
    "import torch_ac\n",
    "import tensorboardX\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "import utils\n",
    "from utils import device\n",
    "from model import ACModel\n",
    "\n",
    "import gymnasium as gym\n",
    "sys.path.append('gym-sokoban')\n",
    "import gym_sokoban"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    algo = 'ppo'             # algorithm to use: a2c | ppo (REQUIRED)\n",
    "    # https://github.com/mpSchrader/gym-sokoban/blob/default/docs/variations/Boxoban.md\n",
    "    env = 'Boxoban-Train-v1'    # name of the environment to train on (REQUIRED)\n",
    "    # env = 'MiniGrid-DoorKey-5x5-v0'    # name of the environment to train on (REQUIRED)\n",
    "    \n",
    "    model = None            # name of the model (default: {ENV}_{ALGO}_{TIME})\n",
    "    seed = 1                 # random seed (default: 1)\n",
    "    log_interval = 1         # number of updates between two logs (default: 1)\n",
    "    save_interval = 10       # number of updates between two saves (default: 10, 0 means no saving)\n",
    "    procs = 1               # number of processes (default: 16)\n",
    "    frames = 200000           # number of frames of training (default: 1e7)\n",
    "\n",
    "    # parameters for main algorithm\n",
    "    epochs = 4               # number of epochs for PPO (default: 4)\n",
    "    batch_size = 256         # batch size for PPO (default: 256)\n",
    "    frames_per_proc = 500   # number of frames per process before update (default: 5 for A2C and 128 for PPO)\n",
    "    discount = 0.99          # discount factor (default: 0.99)\n",
    "    lr = 0.001               # learning rate (default: 0.001)\n",
    "    gae_lambda = 0.95        # lambda coefficient in GAE formula (default: 0.95, 1 means no gae)\n",
    "    entropy_coef = 0.01      # entropy term coefficient (default: 0.01)\n",
    "    value_loss_coef = 0.5    # value loss term coefficient (default: 0.5)\n",
    "    max_grad_norm = 0.5      # maximum norm of gradient (default: 0.5)\n",
    "    optim_eps = 1e-8         # Adam and RMSprop optimizer epsilon (default: 1e-8)\n",
    "    optim_alpha = 0.99       # RMSprop optimizer alpha (default: 0.99)\n",
    "    clip_eps = 0.2           # clipping epsilon for PPO (default: 0.2)\n",
    "    recurrence = 1           # number of time-steps gradient is backpropagated (default: 1). \n",
    "                             # If > 1, a LSTM is added to the model to have memory.\n",
    "    text = False             # add a GRU to the model to handle text input\n",
    "    \n",
    "    # now I need to increase exploration rate\n",
    "    # the parameters for that (from the ones above) is \n",
    "args = Args()\n",
    "\n",
    "args.mem = args.recurrence > 1\n",
    "\n",
    "# Set run dir\n",
    "date = datetime.datetime.now().strftime(\"%y-%m-%d-%H-%M-%S\")\n",
    "default_model_name = f\"{args.env}_{args.algo}_seed{args.seed}_{date}\"\n",
    "model_name = args.model or default_model_name\n",
    "model_dir = utils.get_model_dir(model_name)\n",
    "\n",
    "# Load loggers and Tensorboard writer\n",
    "txt_logger = utils.get_txt_logger(model_dir)\n",
    "csv_file, csv_logger = utils.get_csv_logger(model_dir)\n",
    "tb_writer = tensorboardX.SummaryWriter(model_dir)\n",
    "\n",
    "# Log command and all script arguments\n",
    "txt_logger.info(\"{}\\n\".format(\" \".join(sys.argv)))\n",
    "txt_logger.info(\"{}\\n\".format(args))\n",
    "\n",
    "# Set seed for all randomness sources\n",
    "utils.seed(args.seed)\n",
    "\n",
    "# Set device\n",
    "txt_logger.info(f\"Device: {device}\\n\")\n",
    "\n",
    "# Load environments\n",
    "envs = []\n",
    "for i in range(args.procs):\n",
    "    env = gym.make(args.env, custom_maps=\"custom_maps/1player_1color\")\n",
    "    env.reset()\n",
    "    envs.append(env)\n",
    "txt_logger.info(\"Environments loaded\\n\")\n",
    "\n",
    "# Load training status\n",
    "try:\n",
    "    status = utils.get_status(model_dir)\n",
    "except OSError:\n",
    "    status = {\"num_frames\": 0, \"update\": 0}\n",
    "txt_logger.info(\"Training status loaded\\n\")\n",
    "\n",
    "# Load observations preprocessor\n",
    "obs_space, preprocess_obss = utils.get_obss_preprocessor(envs[0].observation_space)\n",
    "if \"vocab\" in status:\n",
    "    preprocess_obss.vocab.load_vocab(status[\"vocab\"])\n",
    "txt_logger.info(\"Observations preprocessor loaded\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "\n",
    "acmodel = ACModel(obs_space, envs[0].action_space, args.mem, args.text)\n",
    "if \"model_state\" in status:\n",
    "    acmodel.load_state_dict(status[\"model_state\"])\n",
    "acmodel.to(device)\n",
    "txt_logger.info(\"Model loaded\\n\")\n",
    "txt_logger.info(\"{}\\n\".format(acmodel))\n",
    "\n",
    "# Load algo\n",
    "\n",
    "if args.algo == \"a2c\":\n",
    "    algo = torch_ac.A2CAlgo(envs, acmodel, device, args.frames_per_proc, args.discount, args.lr, args.gae_lambda,\n",
    "                            args.entropy_coef, args.value_loss_coef, args.max_grad_norm, args.recurrence,\n",
    "                            args.optim_alpha, args.optim_eps, preprocess_obss)\n",
    "elif args.algo == \"ppo\":\n",
    "    algo = torch_ac.PPOAlgo(envs, acmodel, device, args.frames_per_proc, args.discount, args.lr, args.gae_lambda,\n",
    "                            args.entropy_coef, args.value_loss_coef, args.max_grad_norm, args.recurrence,\n",
    "                            args.optim_eps, args.clip_eps, args.epochs, args.batch_size, preprocess_obss)\n",
    "else:\n",
    "    raise ValueError(\"Incorrect algorithm name: {}\".format(args.algo))\n",
    "\n",
    "if \"optimizer_state\" in status:\n",
    "    algo.optimizer.load_state_dict(status[\"optimizer_state\"])\n",
    "txt_logger.info(\"Optimizer loaded\\n\")\n",
    "\n",
    "# Train model\n",
    "\n",
    "num_frames = status[\"num_frames\"]\n",
    "update = status[\"update\"]\n",
    "start_time = time.time()\n",
    "\n",
    "while num_frames < args.frames:\n",
    "    # Update model parameters\n",
    "    update_start_time = time.time()\n",
    "    exps, logs1 = algo.collect_experiences()\n",
    "    logs2 = algo.update_parameters(exps)\n",
    "    logs = {**logs1, **logs2}\n",
    "    update_end_time = time.time()\n",
    "\n",
    "    num_frames += logs[\"num_frames\"]\n",
    "    update += 1\n",
    "    \n",
    "    # Print logs\n",
    "    if update % args.log_interval == 0:\n",
    "        fps = logs[\"num_frames\"] / (update_end_time - update_start_time)\n",
    "        duration = int(time.time() - start_time)\n",
    "        return_per_episode = utils.synthesize(logs[\"return_per_episode\"])\n",
    "        rreturn_per_episode = utils.synthesize(logs[\"reshaped_return_per_episode\"])\n",
    "        num_frames_per_episode = utils.synthesize(logs[\"num_frames_per_episode\"])\n",
    "\n",
    "        header = [\"update\", \"frames\", \"FPS\", \"duration\"]\n",
    "        data = [update, num_frames, fps, duration]\n",
    "        header += [\"rreturn_\" + key for key in rreturn_per_episode.keys()]\n",
    "        data += rreturn_per_episode.values()\n",
    "        header += [\"num_frames_\" + key for key in num_frames_per_episode.keys()]\n",
    "        data += num_frames_per_episode.values()\n",
    "        header += [\"curr_cutoff\"]\n",
    "        data += [envs[0].unwrapped.curriculum_cutoff]\n",
    "        header += [\"entropy\", \"value\", \"policy_loss\", \"value_loss\", \"grad_norm\"]\n",
    "        data += [logs[\"entropy\"], logs[\"value\"], logs[\"policy_loss\"], logs[\"value_loss\"], logs[\"grad_norm\"]]\n",
    "\n",
    "        # print which levels are learned well\n",
    "        learned_levels = []\n",
    "        for i in range(envs[0].unwrapped.curriculum_cutoff):\n",
    "            if i not in envs[0].unwrapped.steps_per_level:\n",
    "                most_recent_num_steps_on_this_lvl = 200\n",
    "            else:\n",
    "                most_recent_num_steps_on_this_lvl = envs[0].unwrapped.steps_per_level[i][-1]\n",
    "            if most_recent_num_steps_on_this_lvl < 40:\n",
    "                print(\"█\", end=\"\")\n",
    "                learned_levels.append(1)\n",
    "            else:\n",
    "                print(\" \", end=\"\")\n",
    "                learned_levels.append(0)\n",
    "        print(\">\")\n",
    "\n",
    "        txt_logger.info(\n",
    "            \"U {} | F {:06} | FPS {:04.0f} | D {} | rR:μσmM {:6.2f} {:6.2f} {:6.2f} {:6.2f} | F:μσmM {:3.0f} {:3.0f} {:3.0f} {:3.0f} | curr_cutoff {:2.0f} | H {:.3f} | V {:.3f} | pL {:.3f} | vL {:.3f} | ∇ {:.3f}\"\n",
    "            .format(*data))\n",
    "\n",
    "        header += [\"return_\" + key for key in return_per_episode.keys()]\n",
    "        data += return_per_episode.values()\n",
    "\n",
    "        if status[\"num_frames\"] == 0:\n",
    "            csv_logger.writerow(header)\n",
    "        csv_logger.writerow(data)\n",
    "        csv_file.flush()\n",
    "\n",
    "        for field, value in zip(header, data):\n",
    "            tb_writer.add_scalar(field, value, num_frames)\n",
    "\n",
    "    # Save status\n",
    "    if args.save_interval > 0 and update % args.save_interval == 0:\n",
    "        status = {\"num_frames\": num_frames, \"update\": update,\n",
    "                  \"model_state\": acmodel.state_dict(), \"optimizer_state\": algo.optimizer.state_dict()}\n",
    "        if hasattr(preprocess_obss, \"vocab\"):\n",
    "            status[\"vocab\"] = preprocess_obss.vocab.vocab\n",
    "        utils.save_status(status, model_dir)\n",
    "        # txt_logger.info(\"Status saved\")\n",
    "\n",
    "    # # max number of frames per episode has fallen below 40, increase curriculum cutoff\n",
    "    # if num_frames_per_episode[\"max\"] < 40:\n",
    "    #     envs[0].unwrapped.curriculum_cutoff += 1\n",
    "    #     # txt_logger.info(\"curriculum cutoff increased to {}\".format(envs[0].curriculum_cutoff))\n",
    "    #     # reset the environment (not sure if this is necessary)\n",
    "    #     # envs[0].reset()\n",
    "    # # oldTODO this is imperfect, because in the future it can happen that in some update only the easy levels were chosen\n",
    "    # # just by luck, and that wound prematurely increase the cutoff\n",
    "    # # (although with very generous curriculum that isn't a big problem)\n",
    "    \n",
    "    if np.mean(learned_levels) > 0.9:\n",
    "        envs[0].unwrapped.curriculum_cutoff += 1\n",
    "    \n",
    "    # doneTODO also, the harder levels should be played more, and the easy just enough to not forget them\n",
    "    # TODO use some longer bursts of exploration\n",
    "    # TODO the longer you are in the episode, the higher exploration rate should be\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
